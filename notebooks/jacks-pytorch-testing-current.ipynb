{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Make sure to run the script pickle_full_wildfire_data.py to pickle the full 64 x 64 data. The pickled numpy arrays will be used by the Dataset objects to load in the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(f):\n",
    "    with open(f, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "\n",
    "def new_random_crop(labels, new_crop_size):\n",
    "    crop_map = create_crop_map(len(labels), new_crop_size)\n",
    "    good_indices = find_good_samples(labels, crop_map, new_crop_size)\n",
    "    return crop_map, good_indices\n",
    "\n",
    "\n",
    "def create_crop_map(data_size, crop_size):\n",
    "    # The crop map assigns an x_shift and y_shift to each sample in the main 64 x 64 dataset.\n",
    "    crop_map = []\n",
    "    for i in range(data_size):\n",
    "        # random.randint returns beginning <= n <= end, hence the minus 1.\n",
    "        x_shift = random.randint(0, 64 - crop_size - 1)\n",
    "        y_shift = random.randint(0, 64 - crop_size - 1)\n",
    "        crop_map.append((x_shift, y_shift))\n",
    "    return np.array(crop_map)\n",
    "\n",
    "\n",
    "def get_cropped_sample(index, crop_map, crop_size, data, labels):\n",
    "    x_shift, y_shift = crop_map[index]\n",
    "    cropped_features = data[index, :, x_shift : x_shift + crop_size, y_shift : y_shift + crop_size]\n",
    "    cropped_label = labels[index, x_shift : x_shift + crop_size, y_shift : y_shift + crop_size]\n",
    "        \n",
    "    return cropped_features, cropped_label\n",
    "\n",
    "\n",
    "def find_good_samples(labels, crop_map, crop_size):\n",
    "    # Finds the indices of samples that have no missing data in their labels.\n",
    "    # This is determined AFTER generating a crop map and applying the crop to the original 64 x 64 label.\n",
    "    good_indices = []\n",
    "    for i in range(len(labels)):\n",
    "        x_shift, y_shift = crop_map[i]\n",
    "        if np.all(np.invert(labels[i, x_shift : x_shift + crop_size, y_shift : y_shift + crop_size] == -1)):\n",
    "            good_indices.append(i)\n",
    "    return np.array(good_indices)\n",
    "\n",
    "\n",
    "class WildfireDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_filename, labels_filename):\n",
    "        self.data, self.labels = unpickle(data_filename), unpickle(labels_filename)\n",
    "        self.crop_size = 32\n",
    "        random.seed(1)\n",
    "        self.crop_map, self.good_indices = new_random_crop(self.labels, self.crop_size)\n",
    "        \n",
    "        print(f\"data size: {self.data.nbytes}\")\n",
    "        print(f\"label size: {self.labels.nbytes}\")\n",
    "        print(f\"crop_map size: {self.crop_map.nbytes}\")\n",
    "        print(f\"good_indices size: {self.good_indices.nbytes}\")\n",
    "        print(f\"total size: {self.data.nbytes + self.labels.nbytes + self.crop_map.nbytes + self.good_indices.nbytes}\")\n",
    "        print(\"finished initializing\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.good_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        index = self.good_indices[index]\n",
    "        \n",
    "        cropped_features, cropped_label = get_cropped_sample(index, self.crop_map, self.crop_size, self.data, self.labels)\n",
    "        \n",
    "        # Only keep elevation and previous fire mask\n",
    "        cropped_features = cropped_features[[0, 11], :, :]\n",
    "\n",
    "        sample = (torch.from_numpy(cropped_features), torch.from_numpy(np.expand_dims(cropped_label, axis=0)))\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class AugmentedWildfireDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_filename, labels_filename, transform=None):\n",
    "        self.data, self.labels = unpickle(data_filename), unpickle(labels_filename)\n",
    "        self.crop_size = 32\n",
    "        random.seed(1)\n",
    "        self.crop_map, self.good_indices = new_random_crop(self.labels, self.crop_size)\n",
    "        \n",
    "        self.oversample_indices = self._find_samples_for_oversampling()\n",
    "        \n",
    "        print(f\"data size: {self.data.nbytes}\")\n",
    "        print(f\"label size: {self.labels.nbytes}\")\n",
    "        print(f\"crop_map size: {self.crop_map.nbytes}\")\n",
    "        print(f\"good_indices size: {self.good_indices.nbytes}\")\n",
    "        print(f\"total size: {self.data.nbytes + self.labels.nbytes + self.crop_map.nbytes + self.good_indices.nbytes}\")\n",
    "        print(\"finished initializing\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.good_indices) * 2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        index = self.good_indices[index % len(self.good_indices)] if index < len(self.good_indices) else self._get_random_oversample_index()\n",
    "            \n",
    "        cropped_features, cropped_label = get_cropped_sample(index, self.crop_map, self.crop_size, self.data, self.labels)\n",
    "\n",
    "        # Only keep elevation and previous fire mask\n",
    "        cropped_features = cropped_features[[0, 11], :, :]\n",
    "\n",
    "        # Perform random rotations\n",
    "        rotations = [0, 90, 180, 270]\n",
    "        random_rotation = random.choice(rotations)\n",
    "        cropped_features = torchvision.transforms.functional.rotate(torch.from_numpy(cropped_features), random_rotation)\n",
    "        cropped_label = torchvision.transforms.functional.rotate(torch.from_numpy(np.expand_dims(cropped_label, axis=0)), random_rotation)\n",
    "\n",
    "        sample = (cropped_features, cropped_label)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def _find_samples_for_oversampling(self):\n",
    "        oversample_indices = []\n",
    "        threshold = 0.05 # Desired percentage of fire pixels in the target fire masks\n",
    "        \n",
    "        #print(len(self.good_indices))\n",
    "        for i in range(len(self.good_indices)):\n",
    "            index = self.good_indices[i]\n",
    "            x_shift, y_shift = self.crop_map[index]\n",
    "            cropped_label = self.labels[index, x_shift : x_shift + self.crop_size, y_shift : y_shift + self.crop_size]\n",
    "            unique_target, counts_target = np.unique(cropped_label, return_counts=True)\n",
    "            target_counts_map = {int(unique_target[j]): int(counts_target[j]) for j in range(len(unique_target))}\n",
    "            \n",
    "            for key, value in target_counts_map.items():\n",
    "                if key == 1 and (value/1024) >= threshold:\n",
    "                    oversample_indices.append(index)\n",
    "        #print(len(oversample_indices))\n",
    "        #print(oversample_indices)\n",
    "        return oversample_indices\n",
    "        \n",
    "    \n",
    "    def _get_random_oversample_index(self):\n",
    "        return random.choice(self.oversample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 67485696\n",
      "label size: 5623808\n",
      "crop_map size: 21968\n",
      "good_indices size: 10984\n",
      "total size: 73142456\n",
      "finished initializing\n",
      "14\n",
      "1373\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILENAME = '../data/next-day-wildfire-spread/test.data'\n",
    "TEST_LABELS_FILENAME = '../data/next-day-wildfire-spread/test.labels'\n",
    "\n",
    "test_dataset = WildfireDataset(TEST_DATA_FILENAME, TEST_LABELS_FILENAME)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "print(len(test_loader))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 140, in default_collate\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/home/jack/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 140, in default_collate\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "images, labels = batch\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "original_test_data = unpickle(TEST_DATA_FILENAME)\n",
    "original_test_labels = unpickle(TEST_LABELS_FILENAME)\n",
    "original_test_labels = np.expand_dims(original_test_labels, axis=1)\n",
    "\n",
    "print(original_test_data.shape)\n",
    "print(original_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "TITLES = [\n",
    "    'Elevation',\n",
    "    'Wind\\ndirection',\n",
    "    'Wind\\nvelocity',\n",
    "    'Min\\ntemp',\n",
    "    'Max\\ntemp',\n",
    "    'Humidity',\n",
    "    'Precip',\n",
    "    'Drought',\n",
    "    'Vegetation',\n",
    "    'Population\\ndensity',\n",
    "    'Energy\\nrelease\\ncomponent',\n",
    "    'Previous\\nfire\\nmask',\n",
    "    'Fire\\nmask',\n",
    "    'Predicted\\nfire\\nmask'\n",
    "]\n",
    "\n",
    "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "BOUNDS = [-1, -0.1, 0.1, 1]\n",
    "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "\n",
    "def imshow(sample, titles):\n",
    "    fig = plt.figure(figsize=(15,6.5))\n",
    "    features = sample[0].shape[0]\n",
    "    for i in range(features + 1):\n",
    "        plt.subplot(1, features + 1, i + 1)\n",
    "        plt.title(titles[i], fontsize=13)\n",
    "        if i < features - 1:\n",
    "            plt.imshow(sample[0][i, :, :], cmap='viridis')\n",
    "        if i == features - 1:\n",
    "            plt.imshow(sample[0][-1, :, :], cmap=CMAP, norm=NORM)\n",
    "        if i == features:\n",
    "            plt.imshow(sample[1][0, :, :], cmap=CMAP, norm=NORM)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "imshow((images[0], labels[0]), [\"Elevation\", \"Previous\\nfire\\nmask\", \"Fire\\nmask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torchvision.transforms.functional.rotate(images[0], 90)\n",
    "test_label = torchvision.transforms.functional.rotate(labels[0], 90)\n",
    "\n",
    "imshow((test_image, test_label), [\"Elevation\", \"Previous\\nfire\\nmask\", \"Fire\\nmask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torchvision.transforms.functional.rotate(images[0], 180)\n",
    "test_label = torchvision.transforms.functional.rotate(labels[0], 180)\n",
    "\n",
    "imshow((test_image, test_label), [\"Elevation\", \"Previous\\nfire\\nmask\", \"Fire\\nmask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torchvision.transforms.functional.rotate(images[0], 270)\n",
    "test_label = torchvision.transforms.functional.rotate(labels[0], 270)\n",
    "\n",
    "imshow((test_image, test_label), [\"Elevation\", \"Previous\\nfire\\nmask\", \"Fire\\nmask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow((original_test_data[0], original_test_labels[0]), TITLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural networks that can be used on the dataset\n",
    "# Make sure the training data is scrubbed of any target fire masks that have missing data\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        x = x.reshape(-1, 1, 32, 32)\n",
    "        return x\n",
    "\n",
    "class BinaryClassifierCNN(torch.nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        flattened_conv2_output_dimensions = (image_size//4)**2\n",
    "        super(BinaryClassifierCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(12, 16, 5, 1, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32 * flattened_conv2_output_dimensions, 1024), # 1024 pixels, output represents probability of fire.\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.out(x)\n",
    "        x = x.reshape(-1, 1, 32, 32)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Tutorial for the autoencoder: https://www.youtube.com/watch?v=345wRyqKkQ0\n",
    "class Reshape(torch.nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class Trim(torch.nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Trim, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :32, :32]\n",
    "\n",
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(12, 16, 3, 1, 0), # 32 x 32 -> 30 x 30\n",
    "            torch.nn.LeakyReLU(0.01),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Conv2d(16, 32, 3, 2, 0), # 30 x 30 -> 14 x 14\n",
    "            torch.nn.LeakyReLU(0.01),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Conv2d(32, 32, 3, 2, 0), # 14 x 14 -> 6 x 6\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(1152, 2) # 1152 = 32 * 6  * 6\n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 1152),\n",
    "            Reshape(-1, 32, 6, 6),\n",
    "            torch.nn.ConvTranspose2d(32, 32, 3, 1, 0), # 6 x 6 -> 8 x 8\n",
    "            torch.nn.LeakyReLU(0.01),\n",
    "            torch.nn.ConvTranspose2d(32, 16, 3, 2, 1), # 8 x 8 -> 15 x 15\n",
    "            torch.nn.LeakyReLU(0.01),\n",
    "            torch.nn.ConvTranspose2d(16, 16, 3, 2, 0), # 15 x 15 -> 31 x 31\n",
    "            torch.nn.LeakyReLU(0.01),\n",
    "            torch.nn.ConvTranspose2d(16, 1, 3, 1, 0), # 31 x 31 -> 33 x 33\n",
    "            Trim(),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://github.com/milesial/Pytorch-UNet\n",
    "\n",
    "\n",
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../savedModels/model-UNet-bestLoss-Rank-0.weights\"\n",
    "model = UNet(2, 1, True)\n",
    "\n",
    "#https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/2\n",
    "# original saved file with DataParallel\n",
    "state_dict = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = unpickle(\"../savedModels/model-UNet-train-loss-Rank-0.history\")\n",
    "validation_loss_history = unpickle(\"../savedModels/model-UNet-validation-loss-Rank-0.history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, linestyle=\"solid\")\n",
    "plt.plot(validation_loss_history, linestyle=\"solid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = unpickle(\"../savedModels/model-UNet-train-loss-Rank-1.history\")\n",
    "validation_loss_history = unpickle(\"../savedModels/model-UNet-validation-loss-Rank-1.history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, linestyle=\"solid\")\n",
    "plt.plot(validation_loss_history, linestyle=\"solid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the loaded in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def test_model(test_loader, model):   \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, (batch_inputs, batch_labels) in enumerate(test_loader):\n",
    "            out = model(batch_inputs)\n",
    "            \n",
    "            targets = torch.flatten(batch_labels)\n",
    "            out = torch.flatten(out)\n",
    "            \n",
    "            fire_predictions = torch.round(torch.sigmoid(out))\n",
    "            \n",
    "            y_pred.extend(fire_predictions)\n",
    "            y_true.extend(targets)\n",
    "            \n",
    "            correct += torch.sum(fire_predictions == targets)\n",
    "            incorrect += torch.sum(fire_predictions != targets)\n",
    "            \n",
    "            total += len(targets)\n",
    "    \n",
    "    accuracy = 100 * correct/total\n",
    "    print(f\"Total = {total}\")\n",
    "    print(f\"This should match the total: {correct + incorrect}\")\n",
    "    print(f\"Total correct = {correct}\")\n",
    "    print(f\"Total incorrect = {incorrect}\")\n",
    "    print(f\"Overall accuracy: {accuracy}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print(\"\\nPrecision, Recall, Fscore, Support:\")\n",
    "    print(np.vstack(precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the loaded in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLES = [\n",
    "    'Elevation',\n",
    "    'Wind\\ndirection',\n",
    "    'Wind\\nvelocity',\n",
    "    'Min\\ntemp',\n",
    "    'Max\\ntemp',\n",
    "    'Humidity',\n",
    "    'Precip',\n",
    "    'Drought',\n",
    "    'Vegetation',\n",
    "    'Population\\ndensity',\n",
    "    'Energy\\nrelease\\ncomponent',\n",
    "    'Previous\\nfire\\nmask',\n",
    "    'Fire\\nmask',\n",
    "    'Predicted\\nfire\\nmask'\n",
    "]\n",
    "\n",
    "def plot_5_predictions(loader, model, offset, titles):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(loader))\n",
    "        out = model(batch[0])\n",
    "        fire_predictions = torch.squeeze(torch.round(torch.sigmoid(out)), dim=1).numpy()\n",
    "        \n",
    "        print(fire_predictions.shape)\n",
    "    \n",
    "        n_rows = 5\n",
    "        n_features = batch[0].shape[1]\n",
    "    \n",
    "        CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "        BOUNDS = [-1, -0.1, 0.1, 1]\n",
    "        NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "    \n",
    "        fig = plt.figure(figsize=(15,6.5))\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_features + 2):\n",
    "            plt.subplot(n_rows, n_features + 2, i * (n_features + 2) + j + 1)\n",
    "            if i == 0:\n",
    "                plt.title(titles[j], fontsize=13)\n",
    "            if j < n_features - 1:\n",
    "                plt.imshow(batch[0][i+offset, j, :, :], cmap='viridis')\n",
    "            if j == n_features - 1:\n",
    "                plt.imshow(batch[0][i+offset, -1, :, :], cmap=CMAP, norm=NORM)\n",
    "            if j == n_features:\n",
    "                plt.imshow(batch[1][i+offset, 0, :, :], cmap=CMAP, norm=NORM)\n",
    "            if j > n_features:\n",
    "                plt.imshow(fire_predictions[i+offset], cmap=CMAP, norm=NORM)\n",
    "            plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_titles = [\"Elevation\", \"Previous\\nfire\\nmask\", \"Fire\\nmask\", \"Predicted\\nfire\\mask\"]\n",
    "\n",
    "plot_5_predictions(test_loader, model, offset=15, titles=test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
